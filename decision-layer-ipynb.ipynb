{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13616026,"sourceType":"datasetVersion","datasetId":8653160},{"sourceId":630035,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":474697,"modelId":490598}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics segmentation-models-pytorch albumentations --quiet\n\n\nimport os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom ultralytics import YOLO\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:40:43.492901Z","iopub.execute_input":"2025-11-04T22:40:43.493799Z","iopub.status.idle":"2025-11-04T22:42:10.242111Z","shell.execute_reply.started":"2025-11-04T22:40:43.493762Z","shell.execute_reply":"2025-11-04T22:42:10.241481Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCreating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# SECTION 1: CONFIGURATION\n# ============================================================================\n\nclass Config:\n    \"\"\"Configuration for inference pipeline\"\"\"\n    \n    # Model paths - UPDATE THESE TO YOUR UPLOADED MODELS\n    YOLO_MODEL_PATH = '/kaggle/input/glaucomamodels/tensorflow2/default/1/yolov8l_od_oc_best.pt'\n    RESNET_MODEL_PATH = '/kaggle/input/glaucomamodels/tensorflow2/default/1/best_resnet_model.pth'\n    UNET_VESSEL_MODEL_PATH = '/kaggle/input/glaucomamodels/tensorflow2/default/1/unet_vessel_best.pth'\n    \n    # Test images directory\n    TEST_IMAGES_DIR = '/kaggle/input/testimage/image1734.png'\n    \n    # Inference parameters\n    IMAGE_SIZE = 512\n    YOLO_CONF = 0.25\n    YOLO_IOU = 0.45\n    \n    # Decision fusion weights (adjusted without U-Net OD/OC)\n    WEIGHT_RESNET = 0.60    # Increased from 0.35\n    WEIGHT_YOLO = 0.20      # Increased from 0.15 (quality check)\n    WEIGHT_VESSEL = 0.20    # Increased from 0.10\n    \n    # Classification thresholds\n    THRESHOLD_NORMAL = 0.35\n    THRESHOLD_SUSPICIOUS = 0.65\n\nconfig = Config()\n\nprint(\"=\"*60)\nprint(\"GLAUCOMA DETECTION INFERENCE PIPELINE - BOOK 5\")\nprint(\"=\"*60)\nprint(\"\\nConfiguration:\")\nprint(f\"  YOLO model: {config.YOLO_MODEL_PATH}\")\nprint(f\"  ResNet model: {config.RESNET_MODEL_PATH}\")\nprint(f\"  U-Net Vessel model: {config.UNET_VESSEL_MODEL_PATH}\")\nprint(f\"\\nDecision weights:\")\nprint(f\"  ResNet: {config.WEIGHT_RESNET}\")\nprint(f\"  YOLO: {config.WEIGHT_YOLO}\")\nprint(f\"  Vessels: {config.WEIGHT_VESSEL}\")\nprint(\"=\"*60)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:51:57.247941Z","iopub.execute_input":"2025-11-04T22:51:57.248563Z","iopub.status.idle":"2025-11-04T22:51:57.255105Z","shell.execute_reply.started":"2025-11-04T22:51:57.248542Z","shell.execute_reply":"2025-11-04T22:51:57.254316Z"}},"outputs":[{"name":"stdout","text":"============================================================\nGLAUCOMA DETECTION INFERENCE PIPELINE - BOOK 5\n============================================================\n\nConfiguration:\n  YOLO model: /kaggle/input/glaucomamodels/tensorflow2/default/1/yolov8l_od_oc_best.pt\n  ResNet model: /kaggle/input/glaucomamodels/tensorflow2/default/1/best_resnet_model.pth\n  U-Net Vessel model: /kaggle/input/glaucomamodels/tensorflow2/default/1/unet_vessel_best.pth\n\nDecision weights:\n  ResNet: 0.6\n  YOLO: 0.2\n  Vessels: 0.2\n============================================================\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================================================\n# SECTION 2: DEVICE SETUP\n# ============================================================================\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nDevice: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:03.927877Z","iopub.execute_input":"2025-11-04T22:52:03.928726Z","iopub.status.idle":"2025-11-04T22:52:03.933427Z","shell.execute_reply.started":"2025-11-04T22:52:03.928680Z","shell.execute_reply":"2025-11-04T22:52:03.932842Z"}},"outputs":[{"name":"stdout","text":"\nDevice: cuda\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ============================================================================\n# SECTION 3: PREPROCESSING\n# ============================================================================\n\nclass FundusPreprocessor:\n    \"\"\"Unified preprocessing pipeline\"\"\"\n    \n    def __init__(self, target_size=512, apply_clahe=True, apply_green_channel=True):\n        self.target_size = target_size\n        self.apply_clahe = apply_clahe\n        self.apply_green_channel = apply_green_channel\n    \n    def preprocess(self, image):\n        \"\"\"Main preprocessing function\"\"\"\n        # Resize\n        img = cv2.resize(image, (self.target_size, self.target_size), \n                        interpolation=cv2.INTER_AREA)\n        \n        # Green channel extraction\n        if self.apply_green_channel and len(img.shape) == 3:\n            green_channel = img[:, :, 1]\n        else:\n            green_channel = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n        \n        # CLAHE\n        if self.apply_clahe:\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            enhanced = clahe.apply(green_channel)\n        else:\n            enhanced = green_channel\n        \n        # Normalize\n        normalized = enhanced.astype(np.float32) / 255.0\n        \n        # Return 3-channel\n        return np.stack([normalized]*3, axis=-1)\n\npreprocessor = FundusPreprocessor(target_size=config.IMAGE_SIZE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:08.146683Z","iopub.execute_input":"2025-11-04T22:52:08.146934Z","iopub.status.idle":"2025-11-04T22:52:08.153170Z","shell.execute_reply.started":"2025-11-04T22:52:08.146917Z","shell.execute_reply":"2025-11-04T22:52:08.152363Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# ============================================================================\n# SECTION 4: MODEL LOADERS\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"LOADING MODELS\")\nprint(\"=\"*60)\n\ndef load_yolo_model(model_path):\n    \"\"\"Load YOLO v8 model for OD/OC detection\"\"\"\n    print(f\"Loading YOLO from {model_path}...\")\n    if not Path(model_path).exists():\n        raise FileNotFoundError(f\"YOLO model not found: {model_path}\")\n    model = YOLO(model_path)\n    print(\"✓ YOLO loaded\")\n    return model\n\ndef load_resnet_model(model_path, num_classes=2):\n    \"\"\"Load ResNet50 classifier\"\"\"\n    print(f\"Loading ResNet from {model_path}...\")\n    if not Path(model_path).exists():\n        raise FileNotFoundError(f\"ResNet model not found: {model_path}\")\n    \n    model = models.resnet50(pretrained=False)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = model.to(device)\n    model.eval()\n    print(\"✓ ResNet loaded\")\n    return model\n\ndef load_unet_vessel_model(model_path):\n    \"\"\"Load U-Net for vessel segmentation\"\"\"\n    print(f\"Loading U-Net Vessels from {model_path}...\")\n    if not Path(model_path).exists():\n        raise FileNotFoundError(f\"U-Net Vessel model not found: {model_path}\")\n    \n    model = smp.Unet(\n        encoder_name=\"resnet34\",\n        encoder_weights=None,\n        in_channels=3,\n        classes=1,\n        activation=None\n    )\n\n    # ✅ Load checkpoint\n    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n\n    # ✅ If it’s a full checkpoint, extract the actual model weights\n    if \"model_state_dict\" in checkpoint:\n        state_dict = checkpoint[\"model_state_dict\"]\n    else:\n        state_dict = checkpoint  # in case it’s already a pure state_dict\n\n    model.load_state_dict(state_dict)\n    model = model.to(device)\n    model.eval()\n    print(\"✓ U-Net Vessels loaded\")\n    return model\n\n\n# Load all models\ntry:\n    yolo_model = load_yolo_model(config.YOLO_MODEL_PATH)\n    resnet_model = load_resnet_model(config.RESNET_MODEL_PATH)\n    unet_vessel_model = load_unet_vessel_model(config.UNET_VESSEL_MODEL_PATH)\n    print(\"\\n✅ All models loaded successfully!\")\nexcept FileNotFoundError as e:\n    print(f\"\\n❌ Error: {e}\")\n    print(\"\\nPlease ensure all model files are uploaded to Kaggle:\")\n    print(\"  1. yolov8l_od_oc_best.pt\")\n    print(\"  2. best_resnet_model.pth\")\n    print(\"  3. unet_vessel_best.pth\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:13.216643Z","iopub.execute_input":"2025-11-04T22:52:13.216920Z","iopub.status.idle":"2025-11-04T22:52:14.525713Z","shell.execute_reply.started":"2025-11-04T22:52:13.216899Z","shell.execute_reply":"2025-11-04T22:52:14.525079Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nLOADING MODELS\n============================================================\nLoading YOLO from /kaggle/input/glaucomamodels/tensorflow2/default/1/yolov8l_od_oc_best.pt...\n✓ YOLO loaded\nLoading ResNet from /kaggle/input/glaucomamodels/tensorflow2/default/1/best_resnet_model.pth...\n✓ ResNet loaded\nLoading U-Net Vessels from /kaggle/input/glaucomamodels/tensorflow2/default/1/unet_vessel_best.pth...\n✓ U-Net Vessels loaded\n\n✅ All models loaded successfully!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================================\n# SECTION 5: YOLO INFERENCE\n# ============================================================================\n\ndef yolo_detect_od_oc(image, model, conf=0.25, iou=0.45):\n    \"\"\"\n    Detect OD and OC using YOLO\n    Returns: detection info and quality score\n    \"\"\"\n    results = model(image, conf=conf, iou=iou, verbose=False)\n    \n    detections = {\n        'od_detected': False,\n        'oc_detected': False,\n        'od_bbox': None,\n        'oc_bbox': None,\n        'od_conf': 0.0,\n        'oc_conf': 0.0,\n        'quality_score': 0.0\n    }\n    \n    if len(results) > 0 and results[0].boxes is not None:\n        boxes = results[0].boxes\n        \n        for box in boxes:\n            cls = int(box.cls[0])\n            conf = float(box.conf[0])\n            bbox = box.xyxy[0].cpu().numpy()\n            \n            if cls == 0:  # Optic Disc\n                detections['od_detected'] = True\n                detections['od_bbox'] = bbox\n                detections['od_conf'] = conf\n            elif cls == 1:  # Optic Cup\n                detections['oc_detected'] = True\n                detections['oc_bbox'] = bbox\n                detections['oc_conf'] = conf\n    \n    # Quality score based on detection confidence\n    if detections['od_detected'] and detections['oc_detected']:\n        detections['quality_score'] = (detections['od_conf'] + detections['oc_conf']) / 2\n    elif detections['od_detected']:\n        detections['quality_score'] = detections['od_conf'] *0.5\n    else:\n        detections['quality_score'] = 0.0\n    \n    return detections\n\n# ============================================================================\n# SECTION 6: RESNET INFERENCE\n# ============================================================================\n\ndef resnet_classify(image, model):\n    \"\"\"\n    Classify image using ResNet\n    Returns: probability of glaucoma\n    \"\"\"\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Convert to uint8 if needed\n    if image.dtype != np.uint8:\n        image = (image * 255).astype(np.uint8)\n    \n    img_tensor = transform(image).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        outputs = model(img_tensor)\n        probs = torch.softmax(outputs, dim=1)\n        glaucoma_prob = probs[0, 1].item()  # Probability of class 1 (glaucoma)\n    \n    return glaucoma_prob\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:26.801821Z","iopub.execute_input":"2025-11-04T22:52:26.802171Z","iopub.status.idle":"2025-11-04T22:52:26.811074Z","shell.execute_reply.started":"2025-11-04T22:52:26.802149Z","shell.execute_reply":"2025-11-04T22:52:26.810291Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# ============================================================================\n# SECTION 7: U-NET VESSEL SEGMENTATION\n# ============================================================================\n\ndef unet_segment_vessels(image, model):\n    \"\"\"\n    Segment vessels using U-Net\n    Returns: vessel density score\n    \"\"\"\n    preprocessed = preprocessor.preprocess(image)\n    preprocessed_uint8 = (preprocessed * 255).astype(np.uint8)\n    \n    transform = A.Compose([\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n    \n    img_tensor = transform(image=preprocessed_uint8)['image'].unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        output = model(img_tensor)\n        pred_sigmoid = torch.sigmoid(output).squeeze().cpu().numpy()\n        vessel_mask = (pred_sigmoid > 0.5).astype(np.uint8)\n    \n    # Calculate vessel density (percentage of vessel pixels)\n    vessel_density = np.mean(vessel_mask)\n    \n    return vessel_density, vessel_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:32.061793Z","iopub.execute_input":"2025-11-04T22:52:32.062160Z","iopub.status.idle":"2025-11-04T22:52:32.067793Z","shell.execute_reply.started":"2025-11-04T22:52:32.062138Z","shell.execute_reply":"2025-11-04T22:52:32.067020Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# ============================================================================\n# SECTION 8: DECISION FUSION MODULE\n# ============================================================================\n\ndef fuse_predictions(resnet_prob, yolo_quality, vessel_density):\n    \"\"\"\n    Fuse all model outputs into final decision\n    \n    Inputs:\n        resnet_prob: 0-1, probability from ResNet classifier\n        yolo_quality: 0-1, YOLO detection quality\n        vessel_density: 0-1, vessel coverage percentage\n    \n    Returns:\n        final_score: 0-1, final glaucoma risk score\n        decision: 'Normal' / 'Suspicious' / 'Glaucoma'\n        confidence: confidence level\n        explanation: detailed reasoning\n    \"\"\"\n    \n    # Convert vessel density to risk (lower density = higher risk in glaucoma)\n    # Typical healthy vessel density: 10-15%, glaucoma: 5-8%\n    vessel_risk = 1.0 - (vessel_density / 0.15)  # Normalize assuming 15% is normal\n    vessel_risk = np.clip(vessel_risk, 0.0, 1.0)\n    \n    # Weighted fusion (NO U-Net OD/OC component)\n    final_score = (\n        config.WEIGHT_RESNET * resnet_prob +\n        config.WEIGHT_YOLO * (1.0 - yolo_quality) +  # Lower quality = higher risk\n        config.WEIGHT_VESSEL * vessel_risk\n    )\n    \n    # Decision thresholds\n    if final_score < config.THRESHOLD_NORMAL:\n        decision = \"Normal\"\n        confidence = \"High\" if final_score < 0.25 else \"Moderate\"\n    elif final_score < config.THRESHOLD_SUSPICIOUS:\n        decision = \"Suspicious\"\n        confidence = \"Moderate\"\n    else:\n        decision = \"Glaucoma\"\n        confidence = \"High\" if final_score > 0.75 else \"Moderate\"\n    \n    # Generate explanation\n    explanation = []\n    \n    if resnet_prob > 0.7:\n        explanation.append(f\"Deep learning classifier indicates HIGH glaucoma risk ({resnet_prob:.2f})\")\n    elif resnet_prob > 0.5:\n        explanation.append(f\"Deep learning classifier indicates MODERATE glaucoma risk ({resnet_prob:.2f})\")\n    else:\n        explanation.append(f\"Deep learning classifier indicates LOW glaucoma risk ({resnet_prob:.2f})\")\n    \n    if yolo_quality < 0.5:\n        explanation.append(f\"Image quality is LOW ({yolo_quality:.2f}) - OD/OC detection uncertain\")\n    elif yolo_quality > 0.8:\n        explanation.append(f\"Image quality is GOOD ({yolo_quality:.2f}) - clear optic disc detected\")\n    \n    if vessel_density < 0.08:\n        explanation.append(f\"Vessel density is LOW ({vessel_density:.3f}) - possible vascular dropout (glaucoma sign)\")\n    elif vessel_density < 0.10:\n        explanation.append(f\"Vessel density is BORDERLINE ({vessel_density:.3f})\")\n    else:\n        explanation.append(f\"Vessel density is NORMAL ({vessel_density:.3f})\")\n    \n    explanation.append(\"Note: For CDR measurement, use Book 4 (Standalone CDR Calculator)\")\n    \n    return final_score, decision, confidence, explanation\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:37.457691Z","iopub.execute_input":"2025-11-04T22:52:37.458242Z","iopub.status.idle":"2025-11-04T22:52:37.465361Z","shell.execute_reply.started":"2025-11-04T22:52:37.458219Z","shell.execute_reply":"2025-11-04T22:52:37.464695Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# ============================================================================\n# SECTION 9: COMPLETE INFERENCE PIPELINE\n# ============================================================================\n\ndef predict_glaucoma(image_path, visualize=True):\n    \"\"\"\n    Complete glaucoma detection pipeline\n    \n    Args:\n        image_path: Path to fundus image\n        visualize: Whether to generate visualization\n    \n    Returns:\n        results: Dictionary with all predictions\n        fig: matplotlib figure (if visualize=True)\n    \"\"\"\n    \n    # Load image\n    image = cv2.imread(str(image_path))\n    if image is None:\n        raise ValueError(f\"Could not load image: {image_path}\")\n    \n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Processing: {Path(image_path).name}\")\n    print('='*60)\n    \n    # ---- Step 1: YOLO Detection ----\n    print(\"\\n[1/3] Running YOLO OD/OC detection...\")\n    yolo_results = yolo_detect_od_oc(image_rgb, yolo_model, \n                                     conf=config.YOLO_CONF, iou=config.YOLO_IOU)\n    print(f\"  OD detected: {yolo_results['od_detected']} (conf: {yolo_results['od_conf']:.3f})\")\n    print(f\"  OC detected: {yolo_results['oc_detected']} (conf: {yolo_results['oc_conf']:.3f})\")\n    print(f\"  Quality score: {yolo_results['quality_score']:.3f}\")\n    \n    # ---- Step 2: ResNet Classification ----\n    print(\"\\n[2/3] Running ResNet classification...\")\n    preprocessed_img = preprocessor.preprocess(image_rgb)\n    resnet_prob = resnet_classify(preprocessed_img, resnet_model)\n    print(f\"  Glaucoma probability: {resnet_prob:.3f}\")\n    \n    # ---- Step 3: U-Net Vessel Segmentation ----\n    print(\"\\n[3/3] Running U-Net vessel segmentation...\")\n    vessel_density, vessel_mask = unet_segment_vessels(image_rgb, unet_vessel_model)\n    print(f\"  Vessel density: {vessel_density:.3f}\")\n    \n    # ---- Step 4: Decision Fusion ----\n    print(\"\\n[4/4] Fusing predictions...\")\n    final_score, decision, confidence, explanation = fuse_predictions(\n        resnet_prob, yolo_results['quality_score'], vessel_density\n    )\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"FINAL RESULTS\")\n    print('='*60)\n    print(f\"Decision: {decision}\")\n    print(f\"Confidence: {confidence}\")\n    print(f\"Risk Score: {final_score:.3f}\")\n    print(f\"\\nExplanation:\")\n    for i, exp in enumerate(explanation, 1):\n        print(f\"  {i}. {exp}\")\n    print('='*60)\n    \n    # Compile results\n    results = {\n        'decision': decision,\n        'confidence': confidence,\n        'risk_score': final_score,\n        'resnet_prob': resnet_prob,\n        'vessel_density': vessel_density,\n        'yolo_detections': yolo_results,\n        'explanation': explanation\n    }\n    \n    # ---- Visualization ----\n    if visualize:\n        fig = visualize_results(image_rgb, results, vessel_mask, yolo_results)\n        return results, fig\n    \n    return results, None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:44.696640Z","iopub.execute_input":"2025-11-04T22:52:44.697293Z","iopub.status.idle":"2025-11-04T22:52:44.705664Z","shell.execute_reply.started":"2025-11-04T22:52:44.697266Z","shell.execute_reply":"2025-11-04T22:52:44.704923Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ============================================================================\n# SECTION 10: VISUALIZATION\n# ============================================================================\n\ndef visualize_results(image, results, vessel_mask, yolo_results):\n    \"\"\"Create comprehensive visualization\"\"\"\n    \n    fig = plt.figure(figsize=(18, 10))\n    gs = fig.add_gridspec(2, 4, hspace=0.3, wspace=0.3)\n    \n    # Color map for decision\n    decision_colors = {\n        'Normal': 'green',\n        'Suspicious': 'orange',\n        'Glaucoma': 'red'\n    }\n    decision_color = decision_colors.get(results['decision'], 'gray')\n    \n    # Row 1: Input, Preprocessed, YOLO, Decision\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.imshow(image)\n    ax1.set_title(\"Original Image\", fontsize=12, weight='bold')\n    ax1.axis('off')\n    \n    preprocessed = preprocessor.preprocess(image)\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.imshow(preprocessed)\n    ax2.set_title(\"Preprocessed\", fontsize=12, weight='bold')\n    ax2.axis('off')\n    \n    # YOLO detections\n    ax3 = fig.add_subplot(gs[0, 2])\n    img_yolo = image.copy()\n    if yolo_results['od_bbox'] is not None:\n        x1, y1, x2, y2 = yolo_results['od_bbox'].astype(int)\n        cv2.rectangle(img_yolo, (x1, y1), (x2, y2), (255, 0, 0), 3)\n        cv2.putText(img_yolo, f\"OD: {yolo_results['od_conf']:.2f}\", \n                   (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n    if yolo_results['oc_bbox'] is not None:\n        x1, y1, x2, y2 = yolo_results['oc_bbox'].astype(int)\n        cv2.rectangle(img_yolo, (x1, y1), (x2, y2), (0, 255, 0), 3)\n        cv2.putText(img_yolo, f\"OC: {yolo_results['oc_conf']:.2f}\", \n                   (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n    ax3.imshow(img_yolo)\n    ax3.set_title(\"YOLO Detections\", fontsize=12, weight='bold')\n    ax3.axis('off')\n    \n    # Final Decision\n    ax4 = fig.add_subplot(gs[0, 3])\n    ax4.text(0.5, 0.6, results['decision'], \n            ha='center', va='center', fontsize=32, \n            color=decision_color, weight='bold')\n    ax4.text(0.5, 0.4, f\"Risk: {results['risk_score']:.3f}\", \n            ha='center', va='center', fontsize=16)\n    ax4.text(0.5, 0.3, f\"Confidence: {results['confidence']}\", \n            ha='center', va='center', fontsize=14)\n    ax4.set_xlim(0, 1)\n    ax4.set_ylim(0, 1)\n    ax4.axis('off')\n    ax4.set_title(\"Final Decision\", fontsize=12, weight='bold')\n    \n    # Row 2: Vessel segmentation and metrics\n    ax5 = fig.add_subplot(gs[1, 0])\n    vessel_overlay = cv2.resize(image, vessel_mask.shape)\n    vessel_overlay[vessel_mask == 1] = [255, 255, 0]\n    ax5.imshow(vessel_overlay)\n    ax5.set_title(f\"Vessels (Density: {results['vessel_density']:.3f})\", \n                 fontsize=12, weight='bold')\n    ax5.axis('off')\n    \n    # Metrics\n    ax6 = fig.add_subplot(gs[1, 1:3])\n    metrics_text = f\"\"\"\nMODEL OUTPUTS (No CDR - use Book 4 for CDR)\n{'='*50}\n\nResNet Probability:  {results['resnet_prob']:.3f}\nVessel Density:      {results['vessel_density']:.3f}\nYOLO Quality:        {results['yolo_detections']['quality_score']:.3f}\n\nFUSION WEIGHTS:\n{'='*50}\nResNet Weight:       {config.WEIGHT_RESNET}\nYOLO Weight:         {config.WEIGHT_YOLO}\nVessel Weight:       {config.WEIGHT_VESSEL}\n\nTHRESHOLDS:\n{'='*50}\nNormal:              < {config.THRESHOLD_NORMAL}\nSuspicious:          {config.THRESHOLD_NORMAL} - {config.THRESHOLD_SUSPICIOUS}\nGlaucoma:            ≥ {config.THRESHOLD_SUSPICIOUS}\n    \"\"\"\n    ax6.text(0.05, 0.95, metrics_text, transform=ax6.transAxes,\n            fontsize=10, verticalalignment='top', family='monospace',\n            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n    ax6.axis('off')\n    \n    # Explanation\n    ax7 = fig.add_subplot(gs[1, 3])\n    explanation_text = \"Clinical Interpretation:\\n\" + \"─\" * 30 + \"\\n\"\n    for i, exp in enumerate(results['explanation'], 1):\n        explanation_text += f\"{i}. {exp}\\n\\n\"\n    ax7.text(0.05, 0.95, explanation_text, transform=ax7.transAxes,\n            fontsize=9, verticalalignment='top',\n            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n    ax7.axis('off')\n    \n    plt.suptitle(f\"Glaucoma Detection Report (3-Model Fusion)\", \n                fontsize=14, weight='bold', y=0.98)\n    \n    return fig\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:51.622354Z","iopub.execute_input":"2025-11-04T22:52:51.622822Z","iopub.status.idle":"2025-11-04T22:52:51.636030Z","shell.execute_reply.started":"2025-11-04T22:52:51.622799Z","shell.execute_reply":"2025-11-04T22:52:51.635248Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# ============================================================================\n# SECTION 11: BATCH INFERENCE\n# ============================================================================\n\ndef batch_inference(image_dir, output_csv='batch_results.csv'):\n    \"\"\"\n    Run inference on all images in a directory\n    \n    Args:\n        image_dir: Directory containing fundus images\n        output_csv: Output CSV file path\n    \"\"\"\n    image_dir = Path(image_dir)\n    image_files = (list(image_dir.glob('*.jpg')) + \n                  list(image_dir.glob('*.png')) +\n                  list(image_dir.glob('*.jpeg')))\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"BATCH PROCESSING: {len(image_files)} images\")\n    print('='*60)\n    \n    batch_results = []\n    \n    for img_path in image_files:\n        try:\n            results, _ = predict_glaucoma(str(img_path), visualize=False)\n            \n            batch_results.append({\n                'image': img_path.name,\n                'decision': results['decision'],\n                'confidence': results['confidence'],\n                'risk_score': results['risk_score'],\n                'resnet_prob': results['resnet_prob'],\n                'vessel_density': results['vessel_density'],\n                'yolo_quality': results['yolo_detections']['quality_score']\n            })\n            \n            print(f\"✓ {img_path.name}: {results['decision']} (risk: {results['risk_score']:.3f})\")\n            \n        except Exception as e:\n            print(f\"✗ {img_path.name}: Error - {e}\")\n            batch_results.append({\n                'image': img_path.name,\n                'decision': 'ERROR',\n                'error': str(e)\n            })\n    \n    # Save results\n    import pandas as pd\n    df = pd.DataFrame(batch_results)\n    df.to_csv(output_csv, index=False)\n    print(f\"\\n✓ Results saved to {output_csv}\")\n    \n    # Summary statistics\n    if len(df[df['decision'] != 'ERROR']) > 0:\n        print(\"\\n\" + \"=\"*60)\n        print(\"BATCH SUMMARY\")\n        print(\"=\"*60)\n        print(df['decision'].value_counts())\n        print(f\"\\nAverage Risk Score: {df[df['decision'] != 'ERROR']['risk_score'].mean():.3f}\")\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:52:59.808037Z","iopub.execute_input":"2025-11-04T22:52:59.808483Z","iopub.status.idle":"2025-11-04T22:52:59.815913Z","shell.execute_reply.started":"2025-11-04T22:52:59.808464Z","shell.execute_reply":"2025-11-04T22:52:59.815198Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# ============================================================================\n# SECTION 12: TEST ON SAMPLE IMAGES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING INFERENCE PIPELINE\")\nprint(\"=\"*60)\n\n# Find test images\ntest_dir = Path(config.TEST_IMAGES_DIR)\nif test_dir.exists():\n    test_images = (list(test_dir.glob('*.jpg')) + \n                  list(test_dir.glob('*.png')) +\n                  list(test_dir.glob('*.jpeg')))\n    \n    if len(test_images) > 0:\n        print(f\"\\nFound {len(test_images)} test images\")\n        \n        # Test on first 3 images\n        for img_path in test_images[:3]:\n            try:\n                results, fig = predict_glaucoma(str(img_path), visualize=True)\n                \n                plt.savefig(f'result_{img_path.stem}.png', \n                           dpi=150, bbox_inches='tight')\n                plt.show()\n                \n            except Exception as e:\n                print(f\"\\n✗ Error processing {img_path.name}: {e}\")\n    else:\n        print(\"\\n⚠️ No test images found in test directory\")\n        print(\"Please upload test images to run inference\")\nelse:\n    print(f\"\\n⚠️ Test directory not found: {test_dir}\")\n    print(\"Please upload test images to Kaggle\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✅ INFERENCE PIPELINE READY!\")\nprint(\"=\"*60)\nprint(\"\\nUsage:\")\nprint(\"  # Single image:\")\nprint(\"  results, fig = predict_glaucoma('path/to/image.jpg')\")\nprint(\"\\n  # Batch processing:\")\nprint(\"  df = batch_inference('/kaggle/input/test-images/')\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:53:05.537741Z","iopub.execute_input":"2025-11-04T22:53:05.538327Z","iopub.status.idle":"2025-11-04T22:53:05.547664Z","shell.execute_reply.started":"2025-11-04T22:53:05.538304Z","shell.execute_reply":"2025-11-04T22:53:05.546950Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTESTING INFERENCE PIPELINE\n============================================================\n\n⚠️ No test images found in test directory\nPlease upload test images to run inference\n\n============================================================\n✅ INFERENCE PIPELINE READY!\n============================================================\n\nUsage:\n  # Single image:\n  results, fig = predict_glaucoma('path/to/image.jpg')\n\n  # Batch processing:\n  df = batch_inference('/kaggle/input/test-images/')\n","output_type":"stream"}],"execution_count":32}]}